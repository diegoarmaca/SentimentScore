{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating KSS performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing necessary libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TextIO, List, Union, Dict, Tuple\n",
    "import doctest\n",
    "from sentiment import *\n",
    "from random import shuffle\n",
    "import csv\n",
    "import sys\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Partitioning data into Test and training data\n",
    "\n",
    "Fisrt we created a function to partition the full dataset into test and training data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_dataset(file:TextIO, file_name:str, test_size:float) -> Dict:\n",
    "    \"\"\"Precondition: test_size > 0.0 and < 1.0 (one decimal)\n",
    "    Create two datasets sorted randomly from the original. The test_dataset has the size \n",
    "    requested in test_size, and the trainin_dataset has the remaining size. \n",
    "    Print a message e.g., \"The files: test_data.txt and training_data.txt were created\", \n",
    "    and return a dictionary e.g., {'test': 'test_data.txt', 'training': 'training_data.txt'}\n",
    "    \n",
    "    >>> file_names = partition_dataset(open('full.txt', 'r'), 'data', 0.2)\n",
    "    The files: test_data.txt and training_data.txt were created\n",
    "    >>> file_names\n",
    "    {'test': 'test_data.txt', 'training': 'training_data.txt'}\n",
    "    \"\"\"\n",
    "    \n",
    "    all_reviews = file.readlines()\n",
    "    shuffle(all_reviews)\n",
    "    rating_counts = {}\n",
    "    test_reviews = []\n",
    "    training_reviews = []\n",
    "    for review in all_reviews:\n",
    "        if review[0] in rating_counts:\n",
    "            rating_counts[review[0]].append(review)\n",
    "        else:\n",
    "            rating_counts[review[0]] = [review]\n",
    "    for rating, reviews in rating_counts.items(): \n",
    "        length_of_test_data = round(len(reviews) * (test_size))\n",
    "        test_reviews.extend(reviews[:length_of_test_data])\n",
    "        training_reviews.extend(reviews[length_of_test_data:])\n",
    "    test_file_name = \"test_\" + file_name + \".txt\"\n",
    "    training_file_name = \"training_\" + file_name + \".txt\"\n",
    "    with open(test_file_name, 'w') as test_file:\n",
    "        for row in test_reviews:\n",
    "            test_file.write(row)\n",
    "\n",
    "    with open(training_file_name, 'w') as training_file:\n",
    "        for row in training_reviews:  \n",
    "            training_file.write(row)\n",
    "\n",
    "    print('The files: '+ test_file_name + ' and ' + training_file_name + ' were created')\n",
    "    return {'test':test_file_name, 'training':training_file_name}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_pss(test_file:TextIO, kss: Dict[str, List[int]]) -> Dict:\n",
    "    \"\"\"Create a csv dataset with the comparison of the scores given by the kss model and the original ones. Print the message \"The file: reviews_comparison.csv was created\" and return the dictionary {'file':'reviews_comparison.csv'}\n",
    "    >>> testing_result = testing_pss(open('full.txt', 'r'), kss)\n",
    "    The file: reviews_comparison.csv was created\n",
    "    >>> testing_result\n",
    "    {'file': 'reviews_comparison.csv'}\n",
    "    \"\"\"\n",
    "    review_scores = []\n",
    "    percentage_errors = []\n",
    "    test_reviews = test_file.readlines()\n",
    "    for review in test_reviews:\n",
    "        statement = review[1:].strip()\n",
    "        predicted_rating = statement_pss(review, kss)\n",
    "        original_rating = float(review[0])\n",
    "        \n",
    "        if predicted_rating != None:\n",
    "            absolute_error = round((abs(float(predicted_rating) - original_rating)), 2)\n",
    "            percentage_error = round((absolute_error / max(original_rating, sys.float_info.epsilon)), 2)\n",
    "            percentage_errors.append(percentage_error)\n",
    "            mean_percentage_error = round(sum(percentage_errors)/len(percentage_errors)*100, 2)\n",
    "            review_scores.append([statement, round(predicted_rating, 2), original_rating, absolute_error])\n",
    "        else:\n",
    "            percentage_error = 'Not Found'\n",
    "\n",
    "    with open('reviews_comparison.csv', mode ='w') as comparison_file:\n",
    "        comparison_writer = csv.writer(comparison_file, delimiter=\",\", quotechar='\"', quoting = csv.QUOTE_MINIMAL)\n",
    "        comparison_writer.writerow([\"Mean Absolute Error(MAE): \" + str(mean_percentage_error) + \" %\" ])\n",
    "        comparison_writer.writerow([\"-\",\"-\",\"-\",\"-\",\"-\"])\n",
    "        comparison_writer.writerow([\"Review\", \"Predicted Rating\", \"Original Rating\", \"Absolute Error\"])\n",
    "        for row in review_scores:\n",
    "            comparison_writer.writerow(row)\n",
    "    print('The file: reviews_comparison.csv was created')\n",
    "    return {'file':'reviews_comparison.csv'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Testing the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The files: test_data.txt and training_data.txt were created\n",
      "The file: reviews_comparison.csv was created\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Pick a dataset   \n",
    "    # dataset = 'tiny.txt'\n",
    "    # dataset = 'small.txt'\n",
    "    # dataset = 'medium.txt'\n",
    "    dataset = 'full.txt'\n",
    "\n",
    "    # Use test mode\n",
    "    doctest.testmod()\n",
    "\n",
    "    # Test if the training and test datasets were created\n",
    "    name_datasets = 'data'\n",
    "    with open(dataset, 'r') as file:\n",
    "        file_names = partition_dataset(file, name_datasets, 0.2) \n",
    "\n",
    "    with open(file_names['training'], 'r') as training_file:\n",
    "            kss = extract_kss(training_file)   \n",
    "\n",
    "    with open(file_names['test'], 'r') as test:\n",
    "            testing_pss(test, kss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analysing the results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
