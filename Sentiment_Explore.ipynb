{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating KSS performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing necessary libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TextIO, List, Union, Dict, Tuple\n",
    "import doctest\n",
    "from sentiment import *\n",
    "from random import shuffle\n",
    "import csv\n",
    "import sys\n",
    "import math\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Partitioning data into Test and training data\n",
    "\n",
    "Fisrt we created a function to partition the full dataset into test and training data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_dataset(file:TextIO, file_name:str, test_size:float) -> Dict:\n",
    "    \"\"\"Precondition: test_size > 0.0 and < 1.0 (one decimal)\n",
    "    Create two datasets sorted randomly from the original. The test_dataset has the size\n",
    "    requested in test_size, and the trainin_dataset has the remaining size.\n",
    "    Print a message e.g., \"The files: test_data.txt and training_data.txt were created\",\n",
    "    and return a dictionary e.g., {'test': 'test_data.txt', 'training': 'training_data.txt'}\n",
    "    \n",
    "    >>> file_names = partition_dataset(open('full.txt', 'r'), 'data', 0.2)\n",
    "    The files: test_data.txt and training_data.txt were created\n",
    "    >>> file_names\n",
    "    {'test': 'test_data.txt', 'training': 'training_data.txt'}\n",
    "    \"\"\"\n",
    "    all_reviews = file.readlines()\n",
    "    shuffle(all_reviews)\n",
    "    rating_counts = {}\n",
    "    test_reviews = []\n",
    "    training_reviews = []\n",
    "    for review in all_reviews:\n",
    "        if review[0] in rating_counts:\n",
    "            rating_counts[review[0]].append(review)\n",
    "        else:\n",
    "            rating_counts[review[0]] = [review]\n",
    "    #print(rating_counts['4'])\n",
    "    for rating, reviews in rating_counts.items():\n",
    "        #print(rating, values)\n",
    "        length_of_test_data = round(len(reviews) * (test_size))\n",
    "        test_reviews.extend(reviews[:length_of_test_data])\n",
    "        training_reviews.extend(reviews[length_of_test_data:])\n",
    "    test_file_name = \"test_\" + file_name + \".txt\"\n",
    "    training_file_name = \"training_\" + file_name + \".txt\"\n",
    "\n",
    "    with open(test_file_name, 'w') as test_file:\n",
    "        for row in test_reviews:\n",
    "            test_file.write(row)\n",
    "\n",
    "    with open(training_file_name, 'w') as training_file:\n",
    "        for row in training_reviews: \n",
    "            training_file.write(row)\n",
    "\n",
    "    print('The files: '+ test_file_name + ' and ' + training_file_name + ' were created')\n",
    "    return {'test':test_file_name, 'training':training_file_name}\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_pss(test_file:TextIO, common_words_file:TextIO ,kss: Dict[str, List[int]], name_datasets) -> Dict:\n",
    "    \"\"\"Create a csv dataset with the comparison of the scores given by the kss model and the original ones. Print the message \"The file: reviews_comparison.csv was created\" and return the dictionary {'file':'reviews_comparison.csv'}\n",
    "    \n",
    "    >>> testing_result = testing_pss(open('full.txt', 'r'), kss)\n",
    "    The file: reviews_comparison.csv was created\n",
    "    >>> testing_result\n",
    "    {'file': 'reviews_comparison.csv'}\n",
    "    \"\"\"\n",
    "    common_words_file = common_words_file.read().splitlines()\n",
    "    review_scores = []\n",
    "    absolute_errors = []\n",
    "    test_reviews = test_file.readlines()\n",
    "    \n",
    "    #sharpened variables\n",
    "    kss_sharpened = {}\n",
    "    absolute_errors_sharpened = []  \n",
    "    \n",
    "    ### remove all common words\n",
    "    # for word, value in kss.items():\n",
    "    #     if word not in common_words_file:\n",
    "    #         kss_sharpened[word] = value\n",
    "    \n",
    "    ### Remove all words that appears 20 times more\n",
    "    for word, value in kss.items():\n",
    "        if not value[1] > 20:\n",
    "            kss_sharpened[word] = value\n",
    "    \n",
    "    ### Remove all neutral words in common words list\n",
    "    # neutral_words = []\n",
    "    # for word in common_words_file:\n",
    "    #     if word_kss(word, kss) != None:\n",
    "    #         if judge(word_kss(word, kss)) != \"neutral\":\n",
    "    #             neutral_words.append(word)\n",
    "    # for word, value in kss.items():\n",
    "    #     if word not in neutral_words:\n",
    "    #         kss_sharpened[word] = value\n",
    "    \n",
    "    ### Add more weights to not neutral words\n",
    "    # for word, values in kss.items():\n",
    "    #     if word_kss(word, kss) != None:\n",
    "    #         kss_sharpened[word] = values\n",
    "    # for word, value in kss_sharpened.items():\n",
    "    #     if judge(word_kss(word, kss))!=\"neutral\":\n",
    "    #         kss_sharpened[word] = [(value[0]*1.2),value[1]]\n",
    "    \n",
    "    ### Add more weights to not neutral words without neutral words\n",
    "    # not_neutral_words = []\n",
    "    # for word in common_words_file:\n",
    "    #     if word_kss(word, kss) != None:\n",
    "    #         if judge(word_kss(word, kss)) != \"neutral\":\n",
    "    #             not_neutral_words.append(word)\n",
    "    # for word, value in kss.items():\n",
    "    #     if word not in not_neutral_words:\n",
    "    #         kss_sharpened[word] = [value[0]*1.5,value[1]]\n",
    "            \n",
    "    ### Portion of occurences in all occurences\n",
    "    \n",
    "              \n",
    "    for review in test_reviews:\n",
    "        statement = review[1:].strip()\n",
    "        original_rating = float(review[0])\n",
    "        \n",
    "        # original predicted rating\n",
    "        predicted_rating = statement_pss(review, kss)\n",
    "        if predicted_rating != None:\n",
    "            is_close_val = math.isclose(predicted_rating, original_rating, abs_tol=0.05)\n",
    "            absolute_error = round((abs(float(predicted_rating) - original_rating)), 2)\n",
    "            absolute_errors.append(absolute_error)\n",
    "            mean_absolute_error = round(sum(absolute_errors)/len(absolute_errors), 5)\n",
    "            review_scores.append([statement, round(predicted_rating, 2), round(predicted_rating), original_rating, absolute_error, is_close_val])\n",
    "        \n",
    "        # sharpened predicted rating\n",
    "        predicted_rating_sharpened = statement_pss(review, kss_sharpened)\n",
    "        if predicted_rating_sharpened != None:\n",
    "            absolute_error_sharpened = round((abs(float(predicted_rating_sharpened) - original_rating)), 2)\n",
    "            absolute_errors_sharpened.append(absolute_error_sharpened)\n",
    "            mean_absolute_error_sharpened = round(sum(absolute_errors_sharpened)/len(absolute_errors_sharpened), 5)\n",
    "    if predicted_rating_sharpened != None:\n",
    "        print(name_datasets + \" org rate: \\t\", mean_absolute_error)\n",
    "        print(name_datasets + \" new rate: \\t\", mean_absolute_error_sharpened)\n",
    "       \n",
    "    with open('reviews_'+ name_datasets + '.csv', mode ='w') as comparison_file:\n",
    "        comparison_writer = csv.writer(comparison_file, delimiter=\",\", quotechar='\"', quoting = csv.QUOTE_MINIMAL)\n",
    "        comparison_writer.writerow([\"Mean Absolute Error(MAE): \" + str(mean_absolute_error) ])\n",
    "        comparison_writer.writerow([\"-\",\"-\",\"-\",\"-\",\"-\"])\n",
    "        comparison_writer.writerow([\"Review\", \"PSS Score\", \"Predicted Rating\", \"Original Rating\", \"Absolute Error\", \"Evaluation Result\"])\n",
    "        for row in review_scores:\n",
    "            comparison_writer.writerow(row)\n",
    "\n",
    "    print('The file:' + 'reviews_'+ name_datasets + '.csv' + ' was created')\n",
    "    return {'Mean Absolute Error(MAE): ', mean_absolute_error}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_test(datasets: Dict[str, str], partition_size):\n",
    "    for name in datasets:\n",
    "        with open(datasets[name], 'r') as file:\n",
    "            file_names = partition_dataset(file, name, partition_size)\n",
    "        with open(file_names['training'], 'r') as training_file:\n",
    "                kss = extract_kss(training_file)  \n",
    "        with open(file_names['test'], 'r') as test:\n",
    "            with open(\"most_common_english_words.txt\") as common_words_file:\n",
    "                testing_result = testing_pss(test, common_words_file, kss, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Testing the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The files: test_small.txt and training_small.txt were created\n",
      "small org rate: \t 0.98833\n",
      "small new rate: \t 0.98833\n",
      "The file:reviews_small.csv was created\n",
      "The files: test_medium.txt and training_medium.txt were created\n",
      "medium org rate: \t 1.06163\n",
      "medium new rate: \t 1.05041\n",
      "The file:reviews_medium.csv was created\n",
      "The files: test_full.txt and training_full.txt were created\n",
      "full org rate: \t 1.00452\n",
      "full new rate: \t 0.95001\n",
      "The file:reviews_full.csv was created\n",
      "The files: test_data.txt and training_data.txt were created\n",
      "data org rate: \t 1.00233\n",
      "data new rate: \t 0.9638\n",
      "The file:reviews_data.csv was created\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    #Create a dictionary containing diferent datasets, in order to compare accuracies among each other.\n",
    "    datasets =  {\n",
    "        \"small\"     : \"small.txt\",\n",
    "        \"medium\"    : \"medium.txt\",\n",
    "        \"full\"      : \"full.txt\"\n",
    "    }        \n",
    "    \n",
    "    \n",
    "    execute_test(datasets, 0.2)\n",
    "\n",
    "    # Pick a dataset  \n",
    "    # dataset = 'tiny.txt'\n",
    "    # dataset = 'small.txt'\n",
    "    #dataset = 'medium.txt'\n",
    "    dataset = 'full.txt'\n",
    "\n",
    "     # Test if the training and test datasets were created\n",
    "    name_datasets = 'data'\n",
    "     \n",
    "    with open(dataset, 'r') as file:\n",
    "        file_names = partition_dataset(file, name_datasets, 0.2)\n",
    "\n",
    " \n",
    "\n",
    "    # Training the model with the training dataset created\n",
    "    with open(file_names['training'], 'r') as training_file:\n",
    "        kss = extract_kss(training_file)  \n",
    "\n",
    "\n",
    "    # Testing the results with the test dataset created\n",
    "    with open(file_names['test'], 'r') as test:\n",
    "        with open(\"most_common_english_words.txt\") as common_words_file:\n",
    "            testing_result = testing_pss(test, common_words_file, kss, name_datasets)\n",
    "   \n",
    "    # Use test mode\n",
    "\n",
    "    #doctest.testmod()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analysing the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>PSS Score</th>\n",
       "      <th>Predicted Rating</th>\n",
       "      <th>Original Rating</th>\n",
       "      <th>Absolute Error</th>\n",
       "      <th>Evaluation Result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The tonal shifts are jolting , and though Wen ...</td>\n",
       "      <td>2.37</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.37</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Um , no. .</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>As happily glib and vicious as its characters .</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.29</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Wobbly Senegalese updating of '' Carmen '' whi...</td>\n",
       "      <td>2.47</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.47</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Acting can not be acted .</td>\n",
       "      <td>2.11</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.11</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  PSS Score  \\\n",
       "0  The tonal shifts are jolting , and though Wen ...       2.37   \n",
       "1                                         Um , no. .       1.00   \n",
       "2    As happily glib and vicious as its characters .       1.71   \n",
       "3  Wobbly Senegalese updating of '' Carmen '' whi...       2.47   \n",
       "4                          Acting can not be acted .       2.11   \n",
       "\n",
       "   Predicted Rating  Original Rating  Absolute Error  Evaluation Result  \n",
       "0                 2              2.0            0.37              False  \n",
       "1                 1              2.0            1.00              False  \n",
       "2                 2              2.0            0.29              False  \n",
       "3                 2              2.0            0.47              False  \n",
       "4                 2              2.0            0.11              False  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluated_reviews = pd.read_csv('reviews_full.csv', header=2)\n",
    "evaluated_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x249dc8d54a8>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(evaluated_reviews['Original Rating'],evaluated_reviews['Evaluation Result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
